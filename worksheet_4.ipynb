{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV88GdJsy4Ip"
      },
      "source": [
        "# To - Do Exercise:\n",
        "**Problem - 1: Perform a classification task with knn from scratch.**\n",
        "2.\tLoad the Dataset:\n",
        "\n",
        "•\tRead the dataset into a pandas DataFrame.\n",
        "\n",
        "•\tDisplay the first few rows and perform exploratory data analysis (EDA) to understand the dataset (e.g., check data types, missing values, summary statistics).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqD0YhblyDMN",
        "outputId": "7de659e8-7cb7-4d41-af7d-1f13fb3e2af2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First five rows of the dataset:\n",
            "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0            6      148             72             35        0  33.6   \n",
            "1            1       85             66             29        0  26.6   \n",
            "2            8      183             64              0        0  23.3   \n",
            "3            1       89             66             23       94  28.1   \n",
            "4            0      137             40             35      168  43.1   \n",
            "\n",
            "   DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                     0.627   50        1  \n",
            "1                     0.351   31        0  \n",
            "2                     0.672   32        1  \n",
            "3                     0.167   21        0  \n",
            "4                     2.288   33        1  \n",
            "\n",
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Pregnancies               768 non-null    int64  \n",
            " 1   Glucose                   768 non-null    int64  \n",
            " 2   BloodPressure             768 non-null    int64  \n",
            " 3   SkinThickness             768 non-null    int64  \n",
            " 4   Insulin                   768 non-null    int64  \n",
            " 5   BMI                       768 non-null    float64\n",
            " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
            " 7   Age                       768 non-null    int64  \n",
            " 8   Outcome                   768 non-null    int64  \n",
            "dtypes: float64(2), int64(7)\n",
            "memory usage: 54.1 KB\n",
            "None\n",
            "\n",
            "Summary Statistics:\n",
            "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
            "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
            "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
            "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
            "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
            "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
            "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
            "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
            "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
            "\n",
            "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
            "count  768.000000                768.000000  768.000000  768.000000  \n",
            "mean    31.992578                  0.471876   33.240885    0.348958  \n",
            "std      7.884160                  0.331329   11.760232    0.476951  \n",
            "min      0.000000                  0.078000   21.000000    0.000000  \n",
            "25%     27.300000                  0.243750   24.000000    0.000000  \n",
            "50%     32.000000                  0.372500   29.000000    0.000000  \n",
            "75%     36.600000                  0.626250   41.000000    1.000000  \n",
            "max     67.100000                  2.420000   81.000000    1.000000  \n",
            "\n",
            "Missing Values:\n",
            "Pregnancies                 0\n",
            "Glucose                     0\n",
            "BloodPressure               0\n",
            "SkinThickness               0\n",
            "Insulin                     0\n",
            "BMI                         0\n",
            "DiabetesPedigreeFunction    0\n",
            "Age                         0\n",
            "Outcome                     0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/drive/MyDrive/diabetes.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"First five rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Display dataset information\n",
        "print(\"\\nDataset Information:\")\n",
        "print(data.info())\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(data.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFvyKmGG1d5x"
      },
      "source": [
        "2. Handle Missing Data:\n",
        "\n",
        "• Handle any missing values appropriately, either by dropping or imputing them based on the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9xb8ztQ1s8q",
        "outputId": "007001b7-8373-4660-e003-6211d75f3bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "No missing values found.\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "if data.isnull().sum().any():\n",
        "    # Example: Fill missing numerical values with column mean\n",
        "    data.fillna(data.mean(), inplace=True)\n",
        "    print(\"\\nHandled missing values by imputing with mean.\")\n",
        "else:\n",
        "    print(\"\\nNo missing values found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qMkZT672HUa"
      },
      "source": [
        "3. Feature Engineering:\n",
        "\n",
        "• Separate the feature matrix (X) and target variable (y).\n",
        "\n",
        "• Perform a train - test split from scratch using a 70% − 30% ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrOEuW821_NY",
        "outputId": "4c2c714e-e657-48cc-b0da-b7a7886c7997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset successfully split into training and test sets.\n"
          ]
        }
      ],
      "source": [
        "# Separate features and target\n",
        "X = data.drop('Outcome', axis=1)\n",
        "y = data['Outcome']\n",
        "\n",
        "# Train-test split\n",
        "def train_test_split_manual(X, y, test_ratio=0.3, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    indices = np.random.permutation(len(X))\n",
        "    test_size = int(len(X) * test_ratio)\n",
        "    test_indices = indices[:test_size]\n",
        "    train_indices = indices[test_size:]\n",
        "    return X.iloc[train_indices], X.iloc[test_indices], y.iloc[train_indices], y.iloc[test_indices]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split_manual(X, y)\n",
        "print(\"\\nDataset successfully split into training and test sets.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lekDUQ2A2K-9"
      },
      "source": [
        "4. Implement KNN:\n",
        "\n",
        "• Build the KNN algorithm from scratch (no libraries like sickit-learn for KNN).\n",
        "\n",
        "• Compute distances using Euclidean distance.\n",
        "\n",
        "• Write functions for:\n",
        "\n",
        "– Predicting the class for a single query.\n",
        "\n",
        "– Predicting classes for all test samples.\n",
        "\n",
        "• Evaluate the performance using accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL7l7kxg2pdx",
        "outputId": "b5d9b626-748d-4b2c-f52a-4eef35f5892b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy of the KNN model (k=3): 67.39%\n"
          ]
        }
      ],
      "source": [
        "# Euclidean distance function\n",
        "def euclidean_distance(point1, point2):\n",
        "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
        "\n",
        "# Predict the class for a single query\n",
        "def knn_predict_single(query, X_train, y_train, k=3):\n",
        "    distances = []\n",
        "    for i in range(len(X_train)):\n",
        "        dist = euclidean_distance(query, X_train.iloc[i])\n",
        "        distances.append((dist, y_train.iloc[i]))\n",
        "    distances.sort(key=lambda x: x[0])  # Sort by distance\n",
        "    nearest_neighbors = [label for _, label in distances[:k]]\n",
        "    prediction = max(set(nearest_neighbors), key=nearest_neighbors.count)  # Majority vote\n",
        "    return prediction\n",
        "\n",
        "# Predict classes for all test samples\n",
        "def knn_predict(X_test, X_train, y_train, k=3):\n",
        "    predictions = []\n",
        "    for i in range(len(X_test)):\n",
        "        pred = knn_predict_single(X_test.iloc[i], X_train, y_train, k)\n",
        "        predictions.append(pred)\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Compute accuracy\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    correct_predictions = np.sum(y_true.values == y_pred)\n",
        "    accuracy = (correct_predictions / len(y_true)) * 100\n",
        "    return accuracy\n",
        "\n",
        "# Perform prediction and evaluation\n",
        "k = 3  # Number of neighbors\n",
        "y_pred = knn_predict(X_test, X_train, y_train, k)\n",
        "accuracy = compute_accuracy(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nAccuracy of the KNN model (k={k}): {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEFkQbGv21hT"
      },
      "source": [
        "# Problem - 2 - Experimentation:\n",
        "\n",
        "1. Repeat the Classification Task:\n",
        "\n",
        "• Scale the Feature matrix X.\n",
        "• Use the scaled data for training and testing the kNN Classifier.\n",
        "• Record the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN7PVLMB263x",
        "outputId": "bf21af5d-5853-4434-93a2-1ad4a394f261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature scaling complete. Training and testing data have been standardized.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the feature matrix (X) using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on training data and transform both train and test sets\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert scaled arrays back to DataFrame for compatibility with KNN functions\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "print(\"Feature scaling complete. Training and testing data have been standardized.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUgSaBME3FJQ"
      },
      "source": [
        "2. Comparative Analysis: Compare the Results -\n",
        "\n",
        "• Compare the accuracy and performance of the kNN model on the original dataset from problem 1\n",
        "\n",
        "versus the scaled dataset.\n",
        "\n",
        "• Discuss:\n",
        "– How scaling impacted the KNN performance.\n",
        "– The reason for any observed changes in accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8u0RiVj3OZy",
        "outputId": "848d518c-5950-4140-949a-c7bd71dc9242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the KNN model on scaled data (k=3): 70.87%\n"
          ]
        }
      ],
      "source": [
        "# Perform prediction and evaluation with scaled data\n",
        "y_pred_scaled = knn_predict(X_test_scaled, X_train_scaled, y_train, k=3)\n",
        "accuracy_scaled = compute_accuracy(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy of the KNN model on scaled data (k=3): {accuracy_scaled:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkugrzZg5A5P",
        "outputId": "2d0591b7-c12e-4d0f-c62e-b74ed346ccfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on original data: 67.39%\n",
            "Accuracy on scaled data: 70.87%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy on original data: {accuracy:.2f}%\")\n",
        "print(f\"Accuracy on scaled data: {accuracy_scaled:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M11Emxn93VO9"
      },
      "source": [
        "# Problem - 3 - Experimentation with k:\n",
        "\n",
        "1. Vary the number of neighbors - k:\n",
        "\n",
        "• Run the KNN model on both the original and scaled datasets for a range of:\n",
        "\n",
        "k= 1, 2, 3, . . . 15\n",
        "\n",
        "• For each k, record:\n",
        "– Accuracy.\n",
        "\n",
        "– Time taken to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TxQd_82k3ib3",
        "outputId": "7656d23c-159b-4c6b-8888-f32970298c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experimentation with varying k completed.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Range of k values to test\n",
        "k_values = range(1, 16)\n",
        "\n",
        "# Initialize dictionaries to store results\n",
        "results_original = {\"k\": [], \"accuracy\": [], \"time\": []}\n",
        "results_scaled = {\"k\": [], \"accuracy\": [], \"time\": []}\n",
        "\n",
        "for k in k_values:\n",
        "    # Time and test on original dataset\n",
        "    start_time = time.time()\n",
        "    y_pred_original = knn_predict(X_test, X_train, y_train, k)\n",
        "    elapsed_time_original = time.time() - start_time\n",
        "    accuracy_original = compute_accuracy(y_test, y_pred_original)\n",
        "\n",
        "    # Record results for original dataset\n",
        "    results_original[\"k\"].append(k)\n",
        "    results_original[\"accuracy\"].append(accuracy_original)\n",
        "    results_original[\"time\"].append(elapsed_time_original)\n",
        "\n",
        "    # Time and test on scaled dataset\n",
        "    start_time = time.time()\n",
        "    y_pred_scaled = knn_predict(X_test_scaled, X_train_scaled, y_train, k)\n",
        "    elapsed_time_scaled = time.time() - start_time\n",
        "    accuracy_scaled = compute_accuracy(y_test, y_pred_scaled)\n",
        "\n",
        "    # Record results for scaled dataset\n",
        "    results_scaled[\"k\"].append(k)\n",
        "    results_scaled[\"accuracy\"].append(accuracy_scaled)\n",
        "    results_scaled[\"time\"].append(elapsed_time_scaled)\n",
        "\n",
        "print(\"Experimentation with varying k completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b_DAFu93i5m"
      },
      "source": [
        "2. Visualize the Results:\n",
        "\n",
        "• Plot the following graphs:\n",
        "\n",
        "– k vs. Accuracy for original and scaled datasets.\n",
        "\n",
        "– k vs. Time Taken for original and scaled datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYJsnOab3tsZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(results_original[\"k\"], results_original[\"accuracy\"], label=\"Original Data\", marker='o')\n",
        "plt.plot(results_scaled[\"k\"], results_scaled[\"accuracy\"], label=\"Scaled Data\", marker='o')\n",
        "plt.title(\"k vs. Accuracy\")\n",
        "plt.xlabel(\"Number of Neighbors (k)\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAiyQgeh58N-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(results_original[\"k\"], results_original[\"time\"], label=\"Original Data\", marker='o')\n",
        "plt.plot(results_scaled[\"k\"], results_scaled[\"time\"], label=\"Scaled Data\", marker='o')\n",
        "plt.title(\"k vs. Time Taken\")\n",
        "plt.xlabel(\"Number of Neighbors (k)\")\n",
        "plt.ylabel(\"Time Taken (seconds)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiGdMSm-3uEq"
      },
      "source": [
        "3. Analyze and Discuss:\n",
        "\n",
        "• Discuss how the choice of k affects the accuracy and computational cost.\n",
        "\n",
        "• Identify the optimal k based on your analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jNel6wV6aB-"
      },
      "source": [
        "\n",
        "# Problem 3: Experimentation with k\n",
        "To address the experimentation with varying the number of neighbors, we will follow these steps:\n",
        "\n",
        "1. Vary the Number of Neighbors (k)\n",
        "Steps:\n",
        "Run the KNN model on both the original and scaled datasets for a range of\n",
        "𝑘\n",
        "=\n",
        "1\n",
        ",\n",
        "2\n",
        ",\n",
        "3\n",
        ",\n",
        "…\n",
        ",\n",
        "15\n",
        "k=1,2,3,…,15.\n",
        "Record the accuracy and time taken for each value of\n",
        "𝑘\n",
        "k.\n",
        "Code:\n",
        "python\n",
        "Copy code\n",
        "import time\n",
        "\n",
        "# Range of k values to test\n",
        "k_values = range(1, 16)\n",
        "\n",
        "# Initialize dictionaries to store results\n",
        "results_original = {\"k\": [], \"accuracy\": [], \"time\": []}\n",
        "results_scaled = {\"k\": [], \"accuracy\": [], \"time\": []}\n",
        "\n",
        "for k in k_values:\n",
        "    # Time and test on original dataset\n",
        "    start_time = time.time()\n",
        "    y_pred_original = knn_predict(X_test, X_train, y_train, k)\n",
        "    elapsed_time_original = time.time() - start_time\n",
        "    accuracy_original = compute_accuracy(y_test, y_pred_original)\n",
        "    \n",
        "    # Record results for original dataset\n",
        "    results_original[\"k\"].append(k)\n",
        "    results_original[\"accuracy\"].append(accuracy_original)\n",
        "    results_original[\"time\"].append(elapsed_time_original)\n",
        "    \n",
        "    # Time and test on scaled dataset\n",
        "    start_time = time.time()\n",
        "    y_pred_scaled = knn_predict(X_test_scaled, X_train_scaled, y_train, k)\n",
        "    elapsed_time_scaled = time.time() - start_time\n",
        "    accuracy_scaled = compute_accuracy(y_test, y_pred_scaled)\n",
        "    \n",
        "    # Record results for scaled dataset\n",
        "    results_scaled[\"k\"].append(k)\n",
        "    results_scaled[\"accuracy\"].append(accuracy_scaled)\n",
        "    results_scaled[\"time\"].append(elapsed_time_scaled)\n",
        "\n",
        "print(\"Experimentation with varying k completed.\")\n",
        "2. Visualize the Results\n",
        "Visualization 1:\n",
        "𝑘\n",
        "k vs. Accuracy\n",
        "Plot the accuracy for both original and scaled datasets.\n",
        "python\n",
        "Copy code\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(results_original[\"k\"], results_original[\"accuracy\"], label=\"Original Data\", marker='o')\n",
        "plt.plot(results_scaled[\"k\"], results_scaled[\"accuracy\"], label=\"Scaled Data\", marker='o')\n",
        "plt.title(\"k vs. Accuracy\")\n",
        "plt.xlabel(\"Number of Neighbors (k)\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "Visualization 2:\n",
        "𝑘\n",
        "k vs. Time Taken\n",
        "Plot the time taken for predictions on both original and scaled datasets.\n",
        "python\n",
        "Copy code\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(results_original[\"k\"], results_original[\"time\"], label=\"Original Data\", marker='o')\n",
        "plt.plot(results_scaled[\"k\"], results_scaled[\"time\"], label=\"Scaled Data\", marker='o')\n",
        "plt.title(\"k vs. Time Taken\")\n",
        "plt.xlabel(\"Number of Neighbors (k)\")\n",
        "plt.ylabel(\"Time Taken (seconds)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "3. Analyze and Discuss\n",
        "How the Choice of\n",
        "𝑘\n",
        "k Affects Accuracy\n",
        "Low\n",
        "𝑘\n",
        "k:\n",
        "For\n",
        "𝑘\n",
        "=\n",
        "1\n",
        "k=1, the model is very sensitive to noise, leading to overfitting. Accuracy might fluctuate.\n",
        "Higher\n",
        "𝑘\n",
        "k:\n",
        "As\n",
        "𝑘\n",
        "k increases, the predictions are based on more neighbors, smoothing the decision boundary.\n",
        "Accuracy might increase up to an optimal\n",
        "𝑘\n",
        "k, after which it may drop due to underfitting.\n",
        "How the Choice of\n",
        "𝑘\n",
        "k Affects Computational Cost\n",
        "Low\n",
        "𝑘\n",
        "k:\n",
        "Computational cost is relatively low because fewer neighbors need to be identified.\n",
        "Higher\n",
        "𝑘\n",
        "k:\n",
        "Time taken increases as more neighbors must be considered, especially with a large dataset.\n",
        "Optimal\n",
        "𝑘\n",
        "k\n",
        "Identify the\n",
        "𝑘\n",
        "k value with the highest accuracy while balancing computational cost.\n",
        "The optimal\n",
        "𝑘\n",
        "k often lies between a range of values (e.g., 3 to 7) depending on the dataset.\n",
        "Example Analysis\n",
        "If the accuracy graph shows a peak at\n",
        "𝑘\n",
        "=\n",
        "5\n",
        "k=5 with acceptable time cost,\n",
        "𝑘\n",
        "=\n",
        "5\n",
        "k=5 can be chosen as the optimal value. Scaling may consistently provide higher accuracy due to better distance calculations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Vary the Number of Neighbors (k)\n",
        "import time\n",
        "\n",
        "# Range of k values to test\n",
        "k_values = range(1, 16)\n",
        "\n",
        "# Initialize dictionaries to store results\n",
        "results_original = {\"k\": [], \"accuracy\": [], \"time\": []}\n",
        "results_scaled = {\"k\": [], \"accuracy\": [], \"time\": []}\n",
        "\n",
        "for k in k_values:\n",
        "    # Time and test on original dataset\n",
        "    start_time = time.time()\n",
        "    y_pred_original = knn_predict(X_test, X_train, y_train, k)\n",
        "    elapsed_time_original = time.time() - start_time\n",
        "    accuracy_original = compute_accuracy(y_test, y_pred_original)\n",
        "\n",
        "    # Record results for original dataset\n",
        "    results_original[\"k\"].append(k)\n",
        "    results_original[\"accuracy\"].append(accuracy_original)\n",
        "    results_original[\"time\"].append(elapsed_time_original)\n",
        "\n",
        "    # Time and test on scaled dataset\n",
        "    start_time = time.time()\n",
        "    y_pred_scaled = knn_predict(X_test_scaled, X_train_scaled, y_train, k)\n",
        "    elapsed_time_scaled = time.time() - start_time\n",
        "    accuracy_scaled = compute_accuracy(y_test, y_pred_scaled)\n",
        "\n",
        "    # Record results for scaled dataset\n",
        "    results_scaled[\"k\"].append(k)\n",
        "    results_scaled[\"accuracy\"].append(accuracy_scaled)\n",
        "    results_scaled[\"time\"].append(elapsed_time_scaled)\n",
        "\n",
        "print(\"Experimentation with varying k completed.\")\n"
      ],
      "metadata": {
        "id": "kdKuZdTjH0pV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Visualize the Results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(results_original[\"k\"], results_original[\"accuracy\"], label=\"Original Data\", marker='o')\n",
        "plt.plot(results_scaled[\"k\"], results_scaled[\"accuracy\"], label=\"Scaled Data\", marker='o')\n",
        "plt.title(\"k vs. Accuracy\")\n",
        "plt.xlabel(\"Number of Neighbors (k)\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Mwc359VcIXJn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualization 2: k k vs. Time Taken\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(results_original[\"k\"], results_original[\"time\"], label=\"Original Data\", marker='o')\n",
        "plt.plot(results_scaled[\"k\"], results_scaled[\"time\"], label=\"Scaled Data\", marker='o')\n",
        "plt.title(\"k vs. Time Taken\")\n",
        "plt.xlabel(\"Number of Neighbors (k)\")\n",
        "plt.ylabel(\"Time Taken (seconds)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Sp3i2_46Izjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMGxg0pi3zp8"
      },
      "source": [
        "# Problem - 4 - Additional Questions {Optional - But Highly Recommended}:\n",
        "• Discuss the challenges of using KNN for large datasets and high-dimensional data.\n",
        "\n",
        "• Suggest strategies to improve the efficiency of KNN (e.g., approximate nearest neighbors, dimensionality\n",
        "reduction)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VMMaG1FW4Exf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# Generate a random dataset\n",
        "data = np.random.random((10000, 128)).astype('float32')\n",
        "\n",
        "# Create an FAISS index\n",
        "index = faiss.IndexFlatL2(128)  # L2 distance (Euclidean)\n",
        "index.add(data)\n",
        "\n",
        "# Query with a random vector\n",
        "query = np.random.random((5, 128)).astype('float32')\n",
        "distances, indices = index.search(query, k=5)\n",
        "\n",
        "print(\"Nearest neighbors:\", indices)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}